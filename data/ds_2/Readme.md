## 일반 데이터 추가

일반 데이터 4000개로 설정 -> 추후 더 필요하면 추가 할 예정


## 데이터 전처리

구두점과의 거리를 만듬

공백이 두 개 이상일 때 하나로 치환

한글, 영어, 구두점(.,?!), 숫자, 줄바꿈(\n)을 제외한 모든 문자를 공백으로 대체

줄바꿈을 포함한 여러 공백을 하나의 공백으로 치환


## 정규화


일반 데이터셋에 있는 ㅋㅋㅋㅋㅋ 같이 자음 모음 같은 경우 2개로 줄임, 자음 모음만  상태



## 불용어 제거

불용어 리스트 590개를 통해 제거



## 형태소 분석

조사인 단어들만  제거한 상태 

 




## 추가로 진행 해야 되는 상황


1. 특수문자 정규화 부분 고려


2. 형태소 분석한 token들에서 문맥에 맞는 새로운 dictiionaly 추가, -> 이름 같은경우 준, 하야 이런식으로 token이 분리되는 현상이 발생함


3.  다른 형태소 분석 도구도 사용해보기 (khai, Komoran, Kkma, Hannanum)
